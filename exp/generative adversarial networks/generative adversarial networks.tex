\documentclass{../exp}
\usepackage{../../ikany}

\title{Generative Adversarial Networks}

\begin{document}
\maketitle

The AI paradigm changes when a new approximating method is discovered.

\section{Maximum likelihood estimate}
If $x\sim f$, which means $\Pr(x^{-1}(A))=\int_Af\,d\lambda$, then the likelihood function $L_n(\theta|-):\Omega^n\to\R_{\ge0}$ for a fixed parameter $\theta$ defined by
\[L_n(\theta|\vec x):=\prod_{i=1}^nf_\theta(x_i)\]
is also a variable.
Write
\[\frac1n\log L_n(\theta|\vec x)=\frac1n\sum_{i=1}^n\log f_\theta(x_i).\]
By the law of large numbers, $\frac1n\log L_n(\theta|-)$ converges to a constant function
\[\E(\log f_\theta(x))=\int_Xf\log f_\theta\]
in measure.

Note that
\begin{align*}
\int_Xf\log f_\theta&\le\int_Xf(f_\theta-1)\\
&=\frac12(\|f\|_2^2+\|f_\theta\|_2^2-\|f-f_\theta\|_2^2)-1.
\end{align*}
Intuitively, bigger $L_n$ is, closer $f_\theta$ and $f$ are.


\section{Gradient descent method}
ascending stochastic gradient


\section{Minimax game}

Minimax is a \emph{decision policy} in a competitive game.

\section{Generative adversarial networks}
Let $X$ be the set of all images having a given pixel size.
Suppose the data distribution $p_{data}$ on $X$ which embodies learning materials is given.
If $x\in X$ is an image that looks like a real human face, then the distribution(mass) function $p_{data}$ has nonnegligible values near the point $x$.
We cannot describe the distribution function $p_{data}$ completely, but only can sample from it.

Let $p_g$ be a distribution on $X$.
The generator $G:\Omega\to X$ is just an arbitrarily taken random variable satisfying $p_g$ for sampling.
The discriminator $D:X\to[0,1]$ is a function
Our purpose is to construct a new method for approximating $p_g\to p_{data}$ by simultaneously updating the discriminator function $D$.

Let $x_i\sim p_{data}$ and $z\sim p_g$ be random variables $\Omega\to X$.
Let $D$ maximize
\[\log D(x)+\log(1-D(z))\]
and $p_g$ minimize
\[\log(1-D(z)).\]


Balancing the convergence rates between $p_g$ and $D$ is important.




\end{document}