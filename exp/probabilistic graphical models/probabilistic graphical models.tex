\documentclass{../exp}
\usepackage{../../ikany}

\title{Probabilistic Graphical Models}

\begin{document}
\maketitle
\tableofcontents




\section{Introduction}

\subsection{Statistical model}
\begin{defn}
A \emph{statistical model} is an approximation scheme for unknown joint probability distribution.
\end{defn}
The general purpose of many statistical models is to \emph{estimate the joint probability distribution} of several random variables.
The joint probability distrbution contains data about relations of the random variables.
For example, suppose our goal is to obtain the most possible value of $Y$ when given $X=x$, and we have already estimated the joint distribution function $f_{X,Y}$.
Then, since the function $y\mapsto f_{X,Y}(x,y)$ describes the distribution of the random variable $Y|X=x$, what we want to find can be defined reasonably as
\[\hat y=\argmax_yf_{X,Y}(x,y).\]

\begin{ex}
A random field, which we have not defined yet, is a way to represent several random variables together with their dependecies.
Therefore, a paramterized random field gives a statistical model.
In this case, training means an approximating process to find the best parameter, usually written as $\beta$.
\end{ex}

\subsection{Random fields}
\begin{defn}[Random field]
A \emph{random field} is a set of random variables parametrized by a topological space or a (directed or undirected) graph.
\end{defn}
\begin{defn}
In this note, a term \emph{random field} or \emph{network} will be used to refer to random fields on a graph.
\end{defn}

\begin{ex}[Markov chain]
Define a graph $G=(V,E)$ and $\cS$ such that:
\begin{align*}
V&=\Z_{t\ge0},\\
E&=\{(t,t+1)\}_{t\in V},\\
\cS&=\text{a finite set}.
\end{align*}
An element $t\in V$ denotes the time $t$.
Then, the set of $\cS$-valued random variables $\{X_t\}_{t\in V}$ indexed by $V$ defines a random field.

The Markov property is given by
\[X_t\indep X_s\mid X_{t-1}\]
for $s\le t$.
Since $\cS$ is finite, alternatively we may rewrite it by
\[\Pr(X_t=x_t\mid X_{t-1}=x_{t-1})=\Pr(X_t=x_t\mid X_{t-1}=x_{t-1},\cdots,X_0=x_0).\]
\end{ex}

\begin{ex}[Maxwell-Boltzman distribution]
Let $X$ be a $\cS$-valued random field on a graph $G=(V,E)$ such that:
\begin{align*}
V&=\{n\}_{n=1}^N,\\
E&=\mt,\\
\cS&=\Z_x^3\x\Z_p^3
\end{align*}
for a large natural number $N$.
The set $V$ is the set of ideal gas particles, and the space of states $\cS$ embodies the discretized phase space.
At each particle $n\in V$ is attached an $\cS$-valued random variable denoted by $X_n$.
Our ultimate goal is to describe the joint probability distribution of $X_n$'s; equivalently, the distribution of a $\cS^V$-valued random variable $X=(X_1,\cdots,X_N)$.
Elements of $\cS^V$ are called \emph{microstates}.

Let $m>0$ be a constant and fix a parameter $\beta=\frac1{k_BT}>0$ which is called \emph{coldness}.
Define the \emph{Boltzmann factor} as a function $\phi_n:\R_{\beta>0}\x\cS\to\R$ such that
\[\phi_n(\beta,i):=e^{-\beta E(i)},\]
where the \emph{energy function} $E:\cS\to\R$ is defined by
\[E(x,p):=\frac{\|p\|^2}{2m}.\]
The assumption for Boltzmann factors states that given $\beta$, the probability for a particle to be in the state $i$ is proportional to the Boltzmann factor: 
\[\Pr(X_n=i_n)\propto\phi_n(\beta,i_n)=e^{-\beta E_{i_n}}\]
for each state $i_n\in\cS$ and a fixed particle $n\in V$.
Thus, we can write
\[\Pr(X_n=i_n)=\frac{\phi_n(\beta,i_n)}{\sum_{j_n\in\cS}\phi_n(\beta,j_n)}=:\frac{\phi_n(\beta,i_n)}{Z_n(\beta)}.\]
If we assume the independence of $X_n$'s, then we get the disjoint probability distribution
\[\Pr(X=i)=\frac{\phi(\beta,i)}{\sum_{j\in\cS^V}\phi(\beta,j)}=:\frac{\phi(\beta,i)}{Z(\beta)},\]
where
\[\phi(\beta,i):=\prod_{n=1}^N\phi_n(\beta,i_n),\qquad Z(\beta)=\prod_{n=1}^NZ_n(\beta).\]
The denominator $Z:\R_{\beta>0}\to\R^+$ is called the \emph{partition function}.
\end{ex}

\begin{ex}[Ising model]
Let $X$ be a $\cS$-valued random field on a graph $G=(V,E)$ such that:
\begin{align*}
V&=\Z^2,\\
E&=\{\,(x,y):\|x-y\|=1\,\},\\
\cS&=\{\pm1\}.
\end{align*}
\end{ex}


\subsection{Independencies}

We review the measure-theoretic definition of probability distribution.
\begin{defn}
Let $\Omega$ be a probability space and $\cS$ a measurable space.
Let $X:\Omega\to\cS$ be a random variable.
The \emph{probability distribution} of $X$ is the pushforward measure $X_*\Pr$ on $\cS$ defined as $X_*\Pr(A)=\Pr(X^{-1}(A))$ for measurable $A\in\cS$.
We often also write $\Pr(X\in A)$ for $X_*\Pr(A)$.
\end{defn}

We mainly deal with several random variables and their joint distribution.
Suppose $X=\{X_n\}_{n=1}^N$ is a set of random variables and let $(\cS_n,\cF_n)$ be the codomain of $X_n$.
\begin{defn}
Radon space?
\[\Pr(X\in A\mid Y\in B):=\lim_{U\downarrow B}\frac{\Pr(X\in A,\,Y\in U)}{\Pr(Y\in U)}.\]
\end{defn}


Consequently, we may find
\begin{itemize}
\item $\Pr(X_1)$ is a probability measure on $\cS_1$,
\item $\Pr(X_1,X_2)$ is a probability measure on $\cS_1\times\cS_2$,
\item $\Pr(X_2\mid X_1)$ is a set of probability measures on $\cS_2$ parametrized by $\cF_1$.
\end{itemize}




\section{Bayesian networks}


\begin{defn}[Bayesian network]
Let $G$ be a directed acyclic graph.
\end{defn}
The graph acts as a parameter space.
We want to investigate mutual effects among the paramtrized random variables.
\begin{thm}[Factorization of probablity]

\end{thm}

\begin{ex}[NBC, Naive Bayesian Classifier]
\end{ex}
\begin{ex}[HMM, Hidden Markov Model]
\end{ex}


\section{Markov networks}

\begin{defn}[Markov network]
\end{defn}
Markov networks are sometimes called MRF, Markov random field.

\begin{ex}[CRF, Conditional Random Field]
Consider a network with a graph $G$ such that vertices are divided into two classes.
\end{ex}

\section{Neural networks}
Probabilistic graphical models provide effective explanations of the neural networks, but neural networks are not confined only to graphical models.
\begin{defn}[Neural network]
\emph{Neural network} cannot be defined mathematically.
It indicates statistical models that can solve problems with a collection of artificial neurons by adjusting connection strength among them.
\end{defn}

\begin{ex}[MLP, Multi-layer Perceptron]

\end{ex}

\begin{ex}[RNN, Recurrent Neural Network]

\end{ex}




\end{document}


\section{Inference}
\subsection{Viterbi algorithm}
\section{Learning}
\subsection{Gradient descent method}



\subsection{Back propagation}
Backpropagation refers to algorithms to train the weight matrices for minimizing the cost function $J$, which does not depend explicitly on any variables except the last layer vector $a^{(n)}$.
However, since $J$ is a function of the weight matrices implicitly, via $a^{(n)}$, we may find the representation of the gradiant of $J$ as viewing it as a function on the space of weight matrices of each given layer.
In other words, we want to find the coefficients of the differential form $dJ$ on the basis $\{dW_{ij}^{(n-1)}\}_{i,j}$, $\{dW_{jk}^{(n-2)}\}_{j,k}$, or $\{dW_{kl}^{(n-3)}\}_{k,l}$, and so on.

Recall the definitions:
\[a_i^{(n)}=\sigma\left(\sum_jW_{ij}^{(n-1)}a_j^{(n-1)}\right).\]
Since the derivative of the sigmoid function is given by $\sigma'=\sigma-\sigma^2$, we can compute the following auxiliary relations
\[\pd{a_i^{(n)}}{a_j^{(n-1)}}=h(a_i^{(n)})W_{ij}^{(n-1)}\c{and}\pd{a_i^{(n)}}{W_{i'j}^{(n-1)}}=\delta_{ii'}h(a_i^{(n)})a_j^{(n-1)},\]
where $h(x)=x-x^2$.

Then, we can compute
\[dJ=\sum_i\pd{J}{a_i^{(n)}}\sum_j\pd{a_i^{(n)}}{W_{ij}^{(n-1)}}\,dW_{ij}^{(n-1)}=\sum_{i,j}\pd{J}{a_i^{(n)}}h(a_i^{(n)})a_j^{(n-1)}\,dW_{ij}^{(n-1)},\]
which implies
\[\del J(W^{(n-1)})=\left[\pd{J}{a_i^{(n)}}h(a_i^{(n)})a_j^{(n-1)}\right]\pd{W_{ij}^{(n-1)}}.\]
Note that it is a function of $a_i$ and $a_j$.
The gradient descent method will take
\[{W_{ij}^{(n-1)}}^+:=W_{ij}^{(n-1)}-\alpha\cdot\pd{J}{a_i^{(n)}}h(a_i^{(n)})a_j^{(n-1)}\]
with a proper parameter $\alpha>0$.

By the same reason,
\begin{align*}
dJ&=\sum_{i,j,k}\pd{J}{a_i^{(n)}}\pd{a_i^{(n)}}{a_j^{(n-1)}}\pd{a_j^{(n-1)}}{W_{jk}^{(n-2)}}\,dW_{jk}^{(n-2)}\\
&=\sum_{i,j,k}\pd{J}{a_i^{(n)}}\cdot h(a_i^{(n)})W_{ij}^{(n-1)}\cdot h(a_j^{(n-1)})a_k^{(n-2)}\,dW_{jk}^{(n-2)},
\end{align*}
which implies
\[\del J(W^{(n-2)})=\left[\sum_i\pd{J}{a_i^{(n)}}\cdot h(a_i^{(n)})W_{ij}^{(n-1)}\cdot h(a_j^{(n-1)})a_k^{(n-2)}\right]\pd{W_{jk}^{(n-2)}}.\]
Therefore, the gradient descent method will take
\begin{align*}
{W_{jk}^{(n-2)}}^+:&=W_{jk}^{(n-2)}-\alpha\cdot\sum_i\pd{J}{a_i^{(n)}}h(a_i^{(n)})W_{ij}^{(n-1)}h(a_j^{(n-1)})a_k^{(n-2)}\\
&=W_{jk}^{(n-2)}+(1-a_j^{(n-1)})a_k^{(n-2)}\sum_i({W_{ij}^{(n-1)}}^+-W_{ij}^{(n-1)})W_{ij}^{(n-1)}.
\end{align*}
In similar way,
\[{W_{kl}^{(n-3)}}^+:=W_{kl}^{(n-3)}+(1-a_k^{(n-2)})a_l^{(n-3)}\sum_i({W_{jk}^{(n-2)}}^+-W_{jk}^{(n-2)})W_{jk}^{(n-2)}(?)\]




\subsection{Maximum likelihood estimate}
\begin{defn}
Let $f$ be a distribution function on a measure space $X$.
Let $\{f_\theta\}_\theta$ be a parametrized family of distrubution functions on $X$.
The \emph{likelihood} $L_n(\theta):\Omega^n\to\R_{\ge0}$ for a fixed parameter $\theta$ is a random variable defined by
\[L_n(\theta):=\prod_{i=1}^nf_\theta(x_i)\]
where $\{x_i\}_i$ is a family of i.i.d. $X$-valued random variables with a distriburion $f$.
\end{defn}
The objective of the likelihood function is to find $\theta$ such that $f_\theta$ approximates the unknown distribution $f$.
Write
\[\frac1n\log L_n(\theta)=\frac1n\sum_{i=1}^n\log f_\theta(x_i).\]
By the law of large numbers, $\frac1n\log L_n(\theta)$ converges to a constant function
\[\E(\log f_\theta(x))=\int_Xf\log f_\theta\]
in measure as $n\to\oo$.
This constant function is exactly what we call \emph{cross entropy}.

The \emph{Kullback-Leibler divergence} is a kind of asymmetric distance function defined from the difference with cross entropy
\[D_{KL}(f\|f_\theta):=\int_Xf\log f-\int_Xf\log f_\theta.\]
It is proved to be always nonnegative by the Jensen inequality: 
\[\int_Xf\log f_\theta-\int_Xf\log f=\int_Xf\log\frac{f_\theta}f\le\log\left(\int_Xf\frac{f_\theta}f\right)=0.\]
Here, we exclude the region $f=0$ from the integration region.
Then, we can say, bigger $L_n(\theta)$ is, closer $f_\theta$ and $f$ are.












\iffalse
\section{Generative adversarial networks}
Let $X$ be the set of all images having a given pixel size.
Suppose the data distribution $p_{data}$ on $X$ which embodies learning materials is given.
If $x\in X$ is an image that looks like a real human face, then the distribution(mass) function $p_{data}$ has nonnegligible values near the point $x$.
We cannot describe the distribution function $p_{data}$ completely, but only can sample from it.

Let $p_g$ be a distribution on $X$.
The generator $G:\Omega\to X$ is just an arbitrarily taken random variable satisfying $p_g$ for sampling.
The discriminator $D:X\to[0,1]$ is a function
Our purpose is to construct a new method for approximating $p_g\to p_{data}$ by simultaneously updating the discriminator function $D$.

Let $x_i\sim p_{data}$ and $z\sim p_g$ be random variables $\Omega\to X$.
Let $D$ maximize
\[\log D(x)+\log(1-D(z))\]
and $p_g$ minimize
\[\log(1-D(z)).\]


Balancing the convergence rates between $p_g$ and $D$ is important.
\fi


